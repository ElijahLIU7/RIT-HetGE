{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"font.family\"] = [\"Times New Roman\"]\n",
    "\n",
    "# 数据\n",
    "eta = []\n",
    "acc = []\n",
    "pre = []\n",
    "rec = []\n",
    "f1 = []\n",
    "auc = []\n",
    "\n",
    "plt.figure(figsize=(8, 7), dpi=300)\n",
    "\n",
    "plt.plot(eta, acc, marker='o', markersize=8, color='#FF4500', linewidth=3.5, label='ACC')\n",
    "plt.plot(eta, f1, marker='d', markersize=8, color='#1E90FF', linewidth=3.5, label='F1')\n",
    "\n",
    "plt.plot(eta, pre, marker='s', markersize=6, color='#32CD32', linewidth=2, alpha=0.7, label='PRE')\n",
    "plt.plot(eta, rec, marker='^', markersize=6, color='#9370DB', linewidth=2, alpha=0.7, label='REC')\n",
    "plt.plot(eta, auc, marker='*', markersize=10, color='#FFD700', linewidth=2, alpha=0.7, label='AUC')\n",
    "\n",
    "for x, y in zip(eta, acc):\n",
    "    plt.annotate(f'{y:.2f}', (x, y), textcoords='offset points',\n",
    "                 xytext=(0, 8), ha='center', fontsize=10)\n",
    "\n",
    "for x, y in zip(eta, f1):\n",
    "    plt.annotate(f'{y:.2f}', (x, y), textcoords='offset points',\n",
    "                 xytext=(0, -12), ha='center', fontsize=10)\n",
    "\n",
    "plt.title('The classification task', fontsize=18)\n",
    "plt.xlabel('$\\eta_1$', fontsize=16)\n",
    "plt.ylabel('Value(%)', fontsize=16)\n",
    "plt.xticks(eta)\n",
    "plt.yticks(np.arange(40, 105, 5))  # 更精细的纵坐标刻度\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.legend(loc='lower right', fontsize=12, framealpha=1)\n",
    "\n",
    "best_acc_idx = acc.index(max(acc))\n",
    "best_f1_idx = f1.index(max(f1))\n",
    "plt.scatter([eta[best_acc_idx]], [max(acc)], color='red', s=100, zorder=5)\n",
    "plt.scatter([eta[best_f1_idx]], [max(f1)], color='blue', s=100, zorder=5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "6429d5869f062eee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = [\"Times New Roman\"]\n",
    "\n",
    "# 回归数据数据\n",
    "rho = []\n",
    "mse = []\n",
    "mae = []\n",
    "r2 = []\n",
    "pcc = []\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 7), dpi=300)\n",
    "\n",
    "ax1.plot(rho, mse, marker='o', markersize=8, color='#0066CC', linewidth=3.5, label='MSE')\n",
    "ax1.plot(rho, mae, marker='s', markersize=6, color='#66B3FF', linewidth=2, alpha=0.7, label='MAE')\n",
    "ax1.set_ylabel('The values of MSE and MAE', fontsize=14)\n",
    "ax1.tick_params(axis='y', labelcolor=\"#0000FF\")\n",
    "ax1.set_ylim(bottom=3, top=8.5)\n",
    "\n",
    "for x, y in zip(rho, mse):\n",
    "    plt.annotate(f'{y:.2f}', (x, y), textcoords='offset points',\n",
    "                 xytext=(0, -12), ha='center', fontsize=10)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(rho, r2, marker='^', markersize=6, color='#008000', linewidth=2, alpha=0.7, label='$R^2$')\n",
    "ax2.plot(rho, pcc, marker='d', markersize=8, color='#80CC80', linewidth=3.5, label='PCC')\n",
    "ax2.set_ylabel('The values of PCC and $R^2$', fontsize=14)\n",
    "ax2.tick_params(axis='y', labelcolor=\"#00FF00\")\n",
    "ax2.set_ylim(bottom=0.35, top=0.9)\n",
    "\n",
    "for x, y in zip(rho, pcc):\n",
    "    plt.annotate(f'{y:.2f}', (x, y), textcoords='offset points',\n",
    "                 xytext=(0, 8), ha='center', fontsize=10)\n",
    "\n",
    "plt.title('The regression task', fontsize=18)\n",
    "ax1.set_xlabel('$\\\\rho_2$', fontsize=16)\n",
    "plt.xticks(rho, fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "plt.legend(lines1 + lines2, labels1 + labels2,\n",
    "           loc='lower right', fontsize=12, framealpha=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('regression_loss.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "e96d58ef6fb7454a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==================== Data input and organization ====================\n",
    "data = {}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==================== Calculate the differences and statistics ====================\n",
    "models = df.columns[1:]\n",
    "n_models = len(models)\n",
    "mean_diff = []\n",
    "ci_lower = []\n",
    "ci_upper = []\n",
    "p_values = []\n",
    "\n",
    "for model in models:\n",
    "    diff = df[\"ours\"] - df[model]\n",
    "    t_stat, p_val = stats.ttest_rel(df[\"ours\"], df[model])\n",
    "    md = diff.mean()\n",
    "    se = diff.std() / np.sqrt(len(diff))\n",
    "    ci = stats.t.interval(0.95, len(diff)-1, loc=md, scale=se)\n",
    "    mean_diff.append(md)\n",
    "    ci_lower.append(ci[0])\n",
    "    ci_upper.append(ci[1])\n",
    "    p_values.append(p_val)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"model\": models,\n",
    "    \"mean_diff\": mean_diff,\n",
    "    \"ci_lower\": ci_lower,\n",
    "    \"ci_upper\": ci_upper,\n",
    "    \"p_value\": p_values\n",
    "})\n",
    "\n",
    "# ==================== Draw a vertical forest map ====================\n",
    "plt.figure(figsize=(8, 10), dpi=300)\n",
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
    "\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "\n",
    "x_pos = np.arange(n_models)\n",
    "\n",
    "plt.errorbar(\n",
    "    x=x_pos,\n",
    "    y=results_df[\"mean_diff\"],\n",
    "    yerr=np.vstack((results_df[\"mean_diff\"] - results_df[\"ci_lower\"],  # 下限差\n",
    "                    results_df[\"ci_upper\"] - results_df[\"mean_diff\"])),  # 上限差\n",
    "    fmt=\"o\",\n",
    "    ecolor=\"#2c7fb8\",\n",
    "    color=\"#668B8B\",\n",
    "    ms=8,\n",
    "    capsize=5,\n",
    "    lw=1.5,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "plt.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1.2, zorder=0)\n",
    "\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.text(x_pos[i], -0.25, row[\"model\"], ha=\"center\", va=\"top\",\n",
    "             rotation=45, fontweight=\"bold\", fontsize=12)\n",
    "\n",
    "    sig = \"*\" * sum([row[\"p_value\"] < 0.001, row[\"p_value\"] < 0.01, row[\"p_value\"] < 0.05])\n",
    "    plt.text(x_pos[i], row[\"ci_upper\"] + 0.2,\n",
    "             f\"Δ={row['mean_diff']:.2f}% \\n(p={row['p_value']:.4f}{sig})\",\n",
    "             ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "models = df.columns[1:]\n",
    "\n",
    "plt.ylabel(\"Performance Difference (ours - Baseline) [Accuracy %]\", fontsize=14)\n",
    "plt.xticks(x_pos, [])\n",
    "plt.ylim(0, max(results_df[\"ci_upper\"]) + 2)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fa7efe8cafb491fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# ==================== Data input and organization ====================\n",
    "data = {}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==================== Calculate the differences and statistics ====================\n",
    "models = df.columns[1:]\n",
    "n_models = len(models)\n",
    "mean_diff = []\n",
    "ci_lower = []\n",
    "ci_upper = []\n",
    "p_values = []\n",
    "hr_values = []\n",
    "hr_lower = []\n",
    "hr_upper = []\n",
    "\n",
    "def create_survival_data(our_data, model_data):\n",
    "    event_times = np.concatenate([our_data, model_data])\n",
    "    events = np.concatenate([np.ones_like(our_data), np.zeros_like(model_data)])\n",
    "    groups = np.concatenate([np.ones_like(our_data), np.zeros_like(model_data)])\n",
    "\n",
    "    survival_df = pd.DataFrame({\n",
    "        'time': event_times,\n",
    "        'event': events,\n",
    "        'group': groups\n",
    "    })\n",
    "\n",
    "    return survival_df\n",
    "\n",
    "for model in models:\n",
    "    diff = df[\"ours\"] - df[model]\n",
    "    t_stat, p_val = stats.ttest_rel(df[\"ours\"], df[model])\n",
    "    md = diff.mean()\n",
    "    se = diff.std() / np.sqrt(len(diff))\n",
    "    ci = stats.t.interval(0.95, len(diff)-1, loc=md, scale=se)\n",
    "\n",
    "    survival_data = create_survival_data(df[\"ours\"], df[model])\n",
    "\n",
    "    results = logrank_test(\n",
    "        survival_data[survival_data['group'] == 1]['time'],\n",
    "        survival_data[survival_data['group'] == 0]['time'],\n",
    "        event_observed_A=survival_data[survival_data['group'] == 1]['event'],\n",
    "        event_observed_B=survival_data[survival_data['group'] == 0]['event']\n",
    "    )\n",
    "\n",
    "    hr = np.exp(results.test_statistic / (len(df) * 2))\n",
    "\n",
    "    z = 1.96\n",
    "    se_log_hr = np.sqrt(1/sum(survival_data['event'] == 1) + 1/sum(survival_data['event'] == 0))\n",
    "    hr_ci_lower = np.exp(np.log(hr) - z * se_log_hr)\n",
    "    hr_ci_upper = np.exp(np.log(hr) + z * se_log_hr)\n",
    "\n",
    "    mean_diff.append(md)\n",
    "    ci_lower.append(ci[0])\n",
    "    ci_upper.append(ci[1])\n",
    "    p_values.append(p_val)\n",
    "    hr_values.append(hr)\n",
    "    hr_lower.append(hr_ci_lower)\n",
    "    hr_upper.append(hr_ci_upper)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Mean Difference (%)\": mean_diff,\n",
    "    \"95% CI Lower (%)\": ci_lower,\n",
    "    \"95% CI Upper (%)\": ci_upper,\n",
    "    \"P-value\": p_values,\n",
    "    \"Hazard Ratio\": hr_values,\n",
    "    \"95% HR Lower\": hr_lower,\n",
    "    \"95% HR Upper\": hr_upper\n",
    "})\n",
    "\n",
    "results_df.to_excel(\"model_comparison_results.xlsx\", index=False)\n",
    "print(\"The result has been successfully output to 'model_comparison_results.xlsx'\")"
   ],
   "id": "26724657dda55d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ==================== Data input and organization ====================\n",
    "data = {}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==================== Calculate the differences and statistics ====================\n",
    "models = df.columns[1:]\n",
    "n_models = len(models)\n",
    "mean_diff = []\n",
    "ci_lower = []\n",
    "ci_upper = []\n",
    "p_values = []\n",
    "\n",
    "for model in models:\n",
    "    diff = df[\"ours\"] - df[model]\n",
    "    t_stat, p_val = stats.ttest_rel(df[\"ours\"], df[model])\n",
    "    md = diff.mean()\n",
    "    se = diff.std() / np.sqrt(len(diff))\n",
    "    ci = stats.t.interval(0.95, len(diff)-1, loc=md, scale=se)\n",
    "    mean_diff.append(md)\n",
    "    ci_lower.append(ci[0])\n",
    "    ci_upper.append(ci[1])\n",
    "    p_values.append(p_val)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"model\": models,\n",
    "    \"mean_diff\": mean_diff,\n",
    "    \"ci_lower\": ci_lower,\n",
    "    \"ci_upper\": ci_upper,\n",
    "    \"p_value\": p_values\n",
    "})\n",
    "\n",
    "# ==================== Draw a vertical forest map ====================\n",
    "plt.figure(figsize=(8, 10), dpi=300)\n",
    "sns.set_style(\"whitegrid\", {\"grid.linestyle\": \"--\"})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "\n",
    "x_pos = np.arange(n_models)\n",
    "\n",
    "plt.errorbar(\n",
    "    x=x_pos,\n",
    "    y=results_df[\"mean_diff\"],\n",
    "    yerr=[results_df[\"mean_diff\"] - results_df[\"ci_lower\"],\n",
    "          results_df[\"ci_upper\"] - results_df[\"mean_diff\"]],\n",
    "    fmt=\"*\",\n",
    "    ecolor=\"#2c7fb8\",\n",
    "    color=\"#DAA520\",\n",
    "    ms=10,\n",
    "    capsize=6,\n",
    "    lw=1.5,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "plt.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1.2, zorder=0)\n",
    "\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.text(x_pos[i], -0.012, row[\"model\"],\n",
    "             ha=\"center\", va=\"top\",\n",
    "             rotation=45,\n",
    "             fontweight=\"bold\",\n",
    "             fontsize=12)\n",
    "\n",
    "    sig = \"*\" * sum([row[\"p_value\"] < 0.001, row[\"p_value\"] < 0.01, row[\"p_value\"] < 0.05])\n",
    "    plt.text(x_pos[i], row[\"ci_upper\"] + 0.01,\n",
    "             f\"Δ={row['mean_diff']:.2f}\\n(p={row['p_value']:.4f}{sig})\",\n",
    "             ha=\"center\", va=\"bottom\",\n",
    "             fontsize=10)\n",
    "\n",
    "plt.ylabel(\"Performance Difference (ours - Baseline) [PCC]\", fontsize=14)\n",
    "plt.xticks(x_pos, [])\n",
    "plt.ylim(0, max(results_df[\"ci_upper\"]) + 0.1)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5c16b134eefe41ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from lifelines.statistics import logrank_test\n",
    "\n",
    "# ==================== 数据输入与整理 ====================\n",
    "data = {}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==================== 计算差异与统计量 ====================\n",
    "models = df.columns[1:]  # 基线模型列表\n",
    "n_models = len(models)\n",
    "mean_diff = []          # 均值差\n",
    "ci_lower = []           # 置信区间下限\n",
    "ci_upper = []           # 置信区间上限\n",
    "p_values = []           # p值\n",
    "hr_values = []          # Hazard Ratio值\n",
    "hr_lower = []           # HR 95%置信区间下限\n",
    "hr_upper = []           # HR 95%置信区间上限\n",
    "\n",
    "# 创建生存分析所需的格式化数据\n",
    "def create_survival_data(our_data, model_data):\n",
    "    # 假设每个值代表一个\"事件时间\"，创建生存分析数据\n",
    "    # 将数据转换为生存分析所需的格式\n",
    "    event_times = np.concatenate([our_data, model_data])\n",
    "    # 创建事件指示符：假设我们的模型总是\"更好\"，因此事件发生\n",
    "    # 而基线模型事件未发生（因为我们的模型表现更好）\n",
    "    events = np.concatenate([np.ones_like(our_data), np.zeros_like(model_data)])\n",
    "    # 创建组别标签：0为基线模型，1为我们的模型\n",
    "    groups = np.concatenate([np.ones_like(our_data), np.zeros_like(model_data)])\n",
    "\n",
    "    # 创建DataFrame\n",
    "    survival_df = pd.DataFrame({\n",
    "        'time': event_times,\n",
    "        'event': events,\n",
    "        'group': groups\n",
    "    })\n",
    "\n",
    "    return survival_df\n",
    "\n",
    "for model in models:\n",
    "    # 计算每折差异\n",
    "    diff = df[\"ours\"] - df[model]\n",
    "    # 配对t检验\n",
    "    t_stat, p_val = stats.ttest_rel(df[\"ours\"], df[model])\n",
    "    # 计算均值差和置信区间\n",
    "    md = diff.mean()\n",
    "    se = diff.std() / np.sqrt(len(diff))\n",
    "    ci = stats.t.interval(0.95, len(diff)-1, loc=md, scale=se)\n",
    "\n",
    "    # 生存分析 - 计算Hazard Ratio\n",
    "    survival_data = create_survival_data(df[\"ours\"], df[model])\n",
    "\n",
    "    # 执行log-rank检验\n",
    "    results = logrank_test(\n",
    "        survival_data[survival_data['group'] == 1]['time'],\n",
    "        survival_data[survival_data['group'] == 0]['time'],\n",
    "        event_observed_A=survival_data[survival_data['group'] == 1]['event'],\n",
    "        event_observed_B=survival_data[survival_data['group'] == 0]['event']\n",
    "    )\n",
    "\n",
    "    # 计算Hazard Ratio (HR)\n",
    "    # HR = exp(coef) 其中coef是Cox模型中组别的系数\n",
    "    # 对于log-rank检验，HR可以近似为风险比\n",
    "    # 这里使用Mantel-Haenszel方法计算HR\n",
    "    hr = np.exp(results.test_statistic / (len(df) * 2))  # 简化的HR计算\n",
    "\n",
    "    # 计算HR的95%置信区间\n",
    "    # 使用Wald方法的近似\n",
    "    z = 1.96  # 95%置信区间的z值\n",
    "    se_log_hr = np.sqrt(1/sum(survival_data['event'] == 1) + 1/sum(survival_data['event'] == 0))\n",
    "    hr_ci_lower = np.exp(np.log(hr) - z * se_log_hr)\n",
    "    hr_ci_upper = np.exp(np.log(hr) + z * se_log_hr)\n",
    "\n",
    "    # 存储结果\n",
    "    mean_diff.append(md)\n",
    "    ci_lower.append(ci[0])\n",
    "    ci_upper.append(ci[1])\n",
    "    p_values.append(p_val)\n",
    "    hr_values.append(hr)\n",
    "    hr_lower.append(hr_ci_lower)\n",
    "    hr_upper.append(hr_ci_upper)\n",
    "\n",
    "# 整理为DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    \"Model\": models,\n",
    "    \"Mean Difference (%)\": mean_diff,\n",
    "    \"95% CI Lower (%)\": ci_lower,\n",
    "    \"95% CI Upper (%)\": ci_upper,\n",
    "    \"P-value\": p_values,\n",
    "    \"Hazard Ratio\": hr_values,\n",
    "    \"95% HR Lower\": hr_lower,\n",
    "    \"95% HR Upper\": hr_upper\n",
    "})\n",
    "\n",
    "# 输出到Excel文件\n",
    "results_df.to_excel(\"model_comparison_results.xlsx\", index=False)\n",
    "print(\"结果已成功输出到 'model_comparison_results.xlsx' 文件\")"
   ],
   "id": "d388c644b8045775",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置全局字体为Times New Roman\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def count_residues(file_pattern, reference_residues=None):\n",
    "    residue_counter_high = defaultdict(int)\n",
    "    residue_counter_low = defaultdict(int)\n",
    "\n",
    "    for filename in glob.glob(file_pattern):\n",
    "        try:\n",
    "            tm_match = re.search(r'Tm-([0-9\\.]+)', filename)\n",
    "            tm_value = float(tm_match.group(1)[:-1])\n",
    "            with open(filename, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader)  # 跳过标题行\n",
    "                for row in reader:\n",
    "                    if len(row) < 2:\n",
    "                        continue\n",
    "                    residue = row[1].strip().capitalize()\n",
    "                    if residue:\n",
    "                        if tm_value >= 60:\n",
    "                            residue_counter_high[residue] += 1\n",
    "                        else:\n",
    "                            residue_counter_low[residue] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {filename} 时出错: {str(e)}\")\n",
    "\n",
    "    # 若提供了参考残基列表，则按参考顺序排序，否则按当前任务的高频残基排序\n",
    "    if reference_residues:\n",
    "        sorted_residues = reference_residues\n",
    "    else:\n",
    "        sorted_residues = sorted(residue_counter_high.keys(),\n",
    "                               key=lambda x: residue_counter_high[x], reverse=True)\n",
    "\n",
    "    return (\n",
    "        {k: residue_counter_high.get(k, 0) for k in sorted_residues},\n",
    "        {k: residue_counter_low.get(k, 0) for k in sorted_residues}\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 处理回归任务（作为排序基准）\n",
    "    reg_file_pattern = 'F:/Downloads/pLDDT/Test_Graph_top_k/*nodes_relations*.csv'\n",
    "    reg_high, reg_low = count_residues(reg_file_pattern)\n",
    "    sorted_residues = list(reg_high.keys())  # 以回归任务的高频残基为排序基准\n",
    "\n",
    "    # 处理分类任务（使用回归任务的排序基准）\n",
    "    class_file_pattern = 'F:/Downloads/pLDDT/Test_Classification_Graph_top_k/*nodes_relations*.csv'\n",
    "    class_high, class_low = count_residues(class_file_pattern, reference_residues=sorted_residues)\n",
    "\n",
    "    # 计算百分比\n",
    "    def get_percentage(data_dict, total):\n",
    "        return {k: (v / total * 100) if total != 0 else 0 for k, v in data_dict.items()}\n",
    "\n",
    "    total_reg_high = sum(reg_high.values())\n",
    "    total_reg_low = sum(reg_low.values())\n",
    "    total_class_high = sum(class_high.values())\n",
    "    total_class_low = sum(class_low.values())\n",
    "\n",
    "    reg_high_pct = get_percentage(reg_high, total_reg_high)\n",
    "    reg_low_pct = get_percentage(reg_low, total_reg_low)\n",
    "    class_high_pct = get_percentage(class_high, total_class_high)\n",
    "    class_low_pct = get_percentage(class_low, total_class_low)\n",
    "\n",
    "    # 准备绘图数据（按排序后的残基顺序）\n",
    "    residues = sorted_residues\n",
    "    data = [\n",
    "        [reg_high_pct[r] for r in residues],     # 回归-高温\n",
    "        [reg_low_pct[r] for r in residues],       # 回归-低温\n",
    "        [class_high_pct[r] for r in residues],   # 分类-高温\n",
    "        [class_low_pct[r] for r in residues]    # 分类-低温\n",
    "    ]\n",
    "\n",
    "    # 绘图参数\n",
    "    fig, ax = plt.subplots(figsize=(16, 9), dpi=300)\n",
    "    bar_width = 0.18  # 单个柱体宽度（4柱总宽度≈0.72，留0.28作为组间距）\n",
    "    group_space = 0.3  # 残基组之间的间距\n",
    "    index = range(len(residues))\n",
    "\n",
    "    # 定义颜色（分类: 红/橙；回归: 蓝/青）\n",
    "    colors = ['#FF4444', '#FF9933', '#3366CC', '#66CCEE']\n",
    "    labels = [\n",
    "        'Regression (Tm≥60°C)',\n",
    "        'Regression (Tm<60°C)',\n",
    "        'Classification (Tm≥60°C)',\n",
    "        'Classification (Tm<60°C)'\n",
    "    ]\n",
    "\n",
    "    # 绘制4个柱体\n",
    "    for i in range(4):\n",
    "        # 计算每个柱体的x位置（组内偏移）\n",
    "        x_pos = [j * (bar_width*4 + group_space) + i*bar_width for j in index]\n",
    "        ax.bar(x_pos, data[i], bar_width, color=colors[i], edgecolor='black', label=labels[i])\n",
    "\n",
    "    # 图表设置\n",
    "    ax.set_xlabel('Residue Type', fontsize=16)\n",
    "    ax.set_ylabel('Percentage (%)', fontsize=16)\n",
    "\n",
    "    # 设置x轴刻度（对准组中心）\n",
    "    ax.set_xticks([j*(bar_width*4 + group_space) + bar_width*1.5 for j in index])\n",
    "    ax.set_xticklabels(residues, rotation=45, ha='center', fontsize=15)\n",
    "\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7, zorder=0)  # 网格置于底层\n",
    "    ax.legend(loc='upper right', fontsize=16)  # 分两列显示图例\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('combined_task_residue_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 统计输出\n",
    "    print(\"\\n===== 任务统计摘要 =====\")\n",
    "    print(\"回归任务：\")\n",
    "    print(f\"Tm≥60°C残基种类: {len(reg_high)}, Tm<60°C残基种类: {len(reg_low)}\")\n",
    "    print(\"分类任务：\")\n",
    "    print(f\"Tm≥60°C残基种类: {len(class_high)}, Tm<60°C残基种类: {len(class_low)}\")\n",
    "    # 输出所有残基的占比详情（表格形式）\n",
    "    print(\"\\n===== 所有残基占比详情 =====\")\n",
    "    # 表头\n",
    "    header = f\"{'残基':<8} | {'回归(Tm≥60%)':>12} | {'回归(Tm<60%)':>12} | {'分类(Tm≥60%)':>12} | {'分类(Tm<60%)':>12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))  # 分隔线\n",
    "\n",
    "    # 逐行输出残基数据\n",
    "    for residue in residues:\n",
    "        rh = reg_high_pct[residue]\n",
    "        rl = reg_low_pct[residue]\n",
    "        ch = class_high_pct[residue]\n",
    "        cl = class_low_pct[residue]\n",
    "        # 格式化每行数据（保留2位小数，右对齐）\n",
    "        row = f\"{residue:<8} | {rh:>12.2f}% | {rl:>12.2f}% | {ch:>12.2f}% | {cl:>12.2f}%\"\n",
    "        print(row)"
   ],
   "id": "f1763c809d5589e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def clean_residue_name(name):\n",
    "    \"\"\"确保残基名称首字母大写，其余字母小写\"\"\"\n",
    "    if not name:\n",
    "        return name\n",
    "    return name[0].upper() + name[1:].lower()\n",
    "\n",
    "def calculate_residue_percentage(residue_counts):\n",
    "    \"\"\"计算每个残基的占比\"\"\"\n",
    "    total = sum(residue_counts.values())\n",
    "    return {res: (count/total)*100 for res, count in residue_counts.items()}\n",
    "\n",
    "\n",
    "def extract_interaction_scores(file_pattern):\n",
    "    # 存储残基的相互作用力分数\n",
    "    interaction_scores = defaultdict(lambda: defaultdict(float))\n",
    "    interaction_counts = defaultdict(lambda: defaultdict(int))\n",
    "    residue_totals = defaultdict(float)  # 存储每个残基的总作用力分数\n",
    "    residue_counts = defaultdict(int)\n",
    "    interactions = ['VDW', 'PIPISTACK', 'HBOND', 'IONIC', 'SSBOND', 'PICATION']\n",
    "\n",
    "    # 读取CSV文件并提取残基和相互作用力分数\n",
    "    for filename in glob.glob(file_pattern):\n",
    "        try:\n",
    "            # 提取文件名中的Tm值\n",
    "            tm_match = re.search(r'Tm-([0-9\\.]+)', filename)\n",
    "            if not tm_match:\n",
    "                continue\n",
    "\n",
    "            tm_str = tm_match.group(1).rstrip('.')\n",
    "            tm_value = float(tm_str)\n",
    "\n",
    "            with open(filename, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader)  # 跳过标题行\n",
    "                for row in reader:\n",
    "                    if len(row) < 2:  # 防止行数据不完整\n",
    "                        continue\n",
    "                    residue = row[1].strip().upper()\n",
    "                    residue_counts[residue] += 1\n",
    "                    if residue:\n",
    "                        for count, i in enumerate(interactions):\n",
    "                            protein = re.search(r'graph_([^_]+)_nodes', filename)\n",
    "                            path = f\"E:/program/GitHub/protein_wang/data/HNet/test2/{i}/{protein.group(1)}.csv\"\n",
    "                            with open(path, 'r') as g:\n",
    "                                reader_1 = csv.reader(g)\n",
    "                                for row_1 in reader_1:\n",
    "                                    if str(int(row_1[0][2:-6])) == row[0] or str(int(row_1[2][2:-6])) == row[0]:\n",
    "                                        interaction_scores[residue][i] += float(row[count+2])\n",
    "                                        interaction_counts[residue][i] += 1\n",
    "                                # print(interaction_counts[residue][i])\n",
    "                                # print(residue)\n",
    "\n",
    "            residue_percent = calculate_residue_percentage(residue_counts)\n",
    "        except Exception as e:\n",
    "                print(f\"处理文件 {filename} 时出错: {str(e)}\")\n",
    "\n",
    "    return interaction_scores, interaction_counts, residue_percent\n",
    "\n",
    "\n",
    "def calculate_normalized_scores(interaction_scores, interaction_counts):\n",
    "    # 计算残基特定的作用力比重\n",
    "    residue_specific_scores = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "    for residue in interaction_scores:\n",
    "        total = interaction_counts[residue]\n",
    "        if total > 0:\n",
    "            for interaction in interaction_scores[residue]:\n",
    "                # 计算该作用力占该残基总作用力的比重\n",
    "                residue_specific_scores[residue][interaction] = (\n",
    "                        interaction_scores[residue][interaction] / total * 100\n",
    "                )  # 转换为百分比\n",
    "\n",
    "    return residue_specific_scores\n",
    "\n",
    "\n",
    "def plot_interaction_heatmap(residue_specific_scores):\n",
    "    if not residue_specific_scores:\n",
    "        print(\"没有可用的数据来绘制热图\")\n",
    "        return\n",
    "\n",
    "    # 创建一个残基列表和相互作用力列表\n",
    "    residues = sorted(residue_specific_scores.keys())\n",
    "    interactions = ['VDW', 'PIPISTACK', 'HBOND', 'IONIC', 'SSBOND', 'PICATION']\n",
    "\n",
    "    # 提取每个残基对应的标准化相互作用力分数\n",
    "    formatted_residues = [clean_residue_name(res) for res in residues]\n",
    "\n",
    "    scores_matrix = np.array(\n",
    "        [[residue_specific_scores[residue][interaction] for interaction in interactions]\n",
    "         for residue in residues]\n",
    "    )\n",
    "\n",
    "    # 使用 seaborn 绘制热图\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    sns.heatmap(\n",
    "        scores_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",  # 显示一位小数\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=interactions,\n",
    "        yticklabels=formatted_residues,\n",
    "        cbar_kws={'label': 'Percentage of Residue Total (%)'}\n",
    "    )\n",
    "    plt.xlabel(\"Interaction Types\", fontsize=14)\n",
    "    plt.ylabel(\"Residue Types\", fontsize=14)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 显示热图\n",
    "    plt.savefig(\"residue_specific_interaction_heatmap.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_to_csv(df, filename=\"residue_interaction_distribution.csv\"):\n",
    "    # 保存为CSV文件\n",
    "    df.to_csv(filename, index=False, float_format=\"%.2f\")\n",
    "    print(f\"数据已保存到 {filename}\")\n",
    "\n",
    "\n",
    "def save_to_dataframe(residue_specific_scores):\n",
    "    # 创建DataFrame\n",
    "    interactions = ['VDW', 'PIPISTACK', 'HBOND', 'IONIC', 'SSBOND', 'PICATION', 'SELF']\n",
    "    residues = sorted(residue_specific_scores.keys())\n",
    "\n",
    "    # 准备数据\n",
    "    data = []\n",
    "    for residue in residues:\n",
    "        row = {'Residue': residue}\n",
    "        for interaction in interactions:\n",
    "            row[interaction] = residue_specific_scores[residue].get(interaction, 0)\n",
    "        data.append(row)\n",
    "\n",
    "    # 创建DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 添加总计行\n",
    "    totals = df[interactions].sum()\n",
    "    totals['Residue'] = 'TOTAL'\n",
    "    df = df.append(totals, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_pattern = \"F:/Downloads/pLDDT/Test_Graph_top_k/*nodes_relations*.csv\"  # 文件路径\n",
    "    # file_pattern = \"F:/dataset/结果/protein_wang/regression/测试集的关键节点/Top_k/*nodes_relations*.csv\"  # 文件路径\n",
    "    # 提取原始分数\n",
    "    interaction_scores, interaction_counts, residue_percent = extract_interaction_scores(file_pattern)\n",
    "\n",
    "    print(\"\\n残基数量占比统计:\")\n",
    "    for res, percent in sorted(residue_percent.items()):\n",
    "        print(f\"{res}: {percent:.2f}%\")\n",
    "\n",
    "    # 计算残基特定的作用力比重\n",
    "    residue_specific_scores = calculate_normalized_scores(interaction_scores, interaction_counts)\n",
    "\n",
    "    # 绘制热图\n",
    "    plot_interaction_heatmap(residue_specific_scores)\n",
    "\n",
    "    # 转换为DataFrame\n",
    "    df = save_to_dataframe(residue_specific_scores)\n",
    "\n",
    "    # 打印表格\n",
    "    print(\"\\n残基相互作用力分布表(百分比):\")\n",
    "    print(df.to_string(index=False, float_format=\"%.2f\"))\n"
   ],
   "id": "559318101e38b050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "residue_percent = {\n",
    "    'His': 0.42, 'Phe': 0.25, 'Trp': 0.98, 'Arg': 13.38, 'Lys': 3.09,\n",
    "    'Cys': 16.00, 'Gln': 25.99, 'Ala': 5.66, 'Leu': 0.46, 'Val': 0.85\n",
    "}\n",
    "\n",
    "interaction_data = {\n",
    "    'Residue': ['His', 'Phe', 'Trp', 'Arg', 'Lys', 'Cys', 'Gln', 'Ala', 'Leu', 'Val'],\n",
    "    'VDW': [18.73, 15.23, 18.06, 15.22, 14.96, 15.29, 15.13, 17.99, 15.06, 14.86],\n",
    "    'PIPISTACK': [1.35, 1.76, 1.24, 0.89, 0.88, 1.05, 1.00, 0.94, 0.52, 1.20],\n",
    "    'HBOND': [16.55, 15.53, 17.82, 14.64, 13.92, 14.38, 14.49, 15.18, 16.76, 15.02],\n",
    "    'IONIC': [0.19, 0.62, 0.76, 0.94, 0.78, 0.82, 0.85, 0.87, 0.57, 0.64],\n",
    "    'SSBOND': [0.04, 0.00, 0.06, 0.02, 0.05, 0.03, 0.03, 0.05, 0.00, 0.00],\n",
    "    'PICATION': [0.38, 0.56, 0.46, 0.32, 0.25, 0.38, 0.32, 0.32, 0.34, 0.30],\n",
    "    'SELF': [62.75, 66.30, 61.60, 67.97, 69.16, 68.05, 68.18, 64.64, 66.76, 67.99]\n",
    "}\n",
    "\n",
    "# ==============================\n",
    "# 数据预处理\n",
    "# ==============================\n",
    "# 图3：转换为DataFrame并按占比排序\n",
    "df_node = pd.DataFrame.from_dict(residue_percent, orient='index', columns=['Abundance (%)'])\n",
    "df_node = df_node.sort_values(by='Abundance (%)', ascending=False)\n",
    "\n",
    "# 图4：转换为DataFrame并设置索引\n",
    "df_edge = pd.DataFrame(interaction_data).set_index('Residue')\n",
    "interactions = ['VDW', 'PIPISTACK', 'HBOND', 'IONIC', 'SSBOND', 'PICATION', 'SELF']\n",
    "df_edge = df_edge[interactions]\n",
    "\n",
    "# ==============================\n",
    "# 绘制复合图表\n",
    "# ==============================\n",
    "plt.figure(figsize=(18, 8))\n",
    "\n",
    "# 左图：图3 - 残基丰度分布（节点角度）\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(\n",
    "    x=df_node.index,\n",
    "    y=df_node['Abundance (%)'],\n",
    "    palette='viridis',\n",
    "    edgecolor='black'\n",
    ")\n",
    "plt.title('(a) Key Residue Abundance in Thermostable Proteins', fontsize=14)\n",
    "plt.xlabel('Residue Type', fontsize=12)\n",
    "plt.ylabel('Abundance Percentage (%)', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# 添加数值标签\n",
    "for p in plt.gca().patches:\n",
    "    height = p.get_height()\n",
    "    plt.text(p.get_x() + p.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# 右图：图4 - 相互作用占比热图（边角度）\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(\n",
    "    df_edge,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='YlGnBu',\n",
    "    cbar_kws={'label': 'Interaction Percentage (%)'}\n",
    ")\n",
    "plt.title('(b) Residue-Interaction Strength Distribution', fontsize=14)\n",
    "plt.xlabel('Interaction Types', fontsize=12)\n",
    "plt.ylabel('Residue Type', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# 添加联动注释（示例：标注关键相互作用）\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.text(1, 8, '*PHE的堆积作用显著', fontsize=10, color='red', ha='center')\n",
    "plt.text(2, 0, '*HIS的氢键主导', fontsize=10, color='red', ha='center')\n",
    "\n",
    "# 全局调整\n",
    "plt.tight_layout()\n",
    "plt.savefig('node_edge_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "59fda37f48175cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 设置全局字体为Times New Roman\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def calculate_normalized_scores():\n",
    "    # 直接使用您提供的新数据\n",
    "    data = \"\"\"\n",
    "    AA\tVDW\tPIPISTACK\tHBOND\tIONIC\tSSBOND\tPICATION\n",
    "    ARG\t0.2565\t0\t0.291166667\t0.240833333\t0\t0.2115\n",
    "    CYS\t0.283639706\t0\t0.31875\t0\t0.397610294\t0\n",
    "    ALA\t0.502484815\t0\t0.497515185\t0\t0\t0\n",
    "    SER\t0.460498648\t0\t0.539501352\t0\t0\t0\n",
    "    GLN\t0.462391369\t0\t0.537608631\t0\t0\t0\n",
    "    ILE\t0.460455362\t0\t0.539544638\t0\t0\t0\n",
    "    THR\t0.450549451\t0\t0.549450549\t0\t0\t0\n",
    "    ASP\t0.313587407\t0\t0.385045568\t0.301367026\t0\t0\n",
    "    ASN\t0.448553534\t0\t0.551446466\t0\t0\t0\n",
    "    GLU\t0.312371558\t0\t0.388409371\t0.299219071\t0\t0\n",
    "    MET\t0.460522366\t0\t0.539477634\t0\t0\t0\n",
    "    HIS\t0.215063319\t0.195178849\t0.253610309\t0.179960009\t0\t0.156187514\n",
    "    LYS\t0.25061065\t0\t0.297182869\t0.238723335\t0\t0.213483146\n",
    "    TRP\t0.254317549\t0.268802228\t0.298746518\t0\t0\t0.178133705\n",
    "    TYR\t0.236837994\t0.269020466\t0.29151695\t0\t0\t0.20262459\n",
    "    VAL\t0.456045605\t0\t0.543954395\t0\t0\t0\n",
    "    GLY\t0.460704607\t0\t0.539295393\t0\t0\t0\n",
    "    LEU\t0.43902439\t0\t0.56097561\t0\t0\t0\n",
    "    PHE\t0.235758514\t0.26377709\t0.301702786\t0\t0\t0.19876161\n",
    "    PRO\t0.438692886\t0\t0.561307114\t0\t0\t0\n",
    "    \"\"\"\n",
    "    # 解析数据\n",
    "    lines = [line.strip() for line in data.split('\\n') if line.strip()]\n",
    "    header = lines[0].split('\\t')\n",
    "    residues = [line.split('\\t')[0] for line in lines[1:]]\n",
    "    interactions = header[1:]\n",
    "\n",
    "    residue_specific_scores = defaultdict(lambda: defaultdict(float))\n",
    "    for line in lines[1:]:\n",
    "        parts = line.split('\\t')\n",
    "        residue = parts[0]\n",
    "        for i, interaction in enumerate(interactions):\n",
    "            score = float(parts[i+1])\n",
    "            residue_specific_scores[residue][interaction] = score * 100  # 转换为百分比\n",
    "\n",
    "    return residue_specific_scores, residues, interactions\n",
    "\n",
    "def plot_interaction_heatmap(residue_specific_scores, residues, interactions):\n",
    "    if not residue_specific_scores:\n",
    "        print(\"无可用数据生成热图。\")\n",
    "        return\n",
    "\n",
    "    # 构建分数矩阵\n",
    "    scores_matrix = np.array([\n",
    "        [residue_specific_scores[residue][interaction] for interaction in interactions]\n",
    "        for residue in residues\n",
    "    ])\n",
    "\n",
    "    # 绘制热图\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    sns.heatmap(\n",
    "        scores_matrix,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"YlGnBu\",\n",
    "        xticklabels=interactions,\n",
    "        yticklabels=residues,\n",
    "        cbar_kws={'label': 'Proportion of residue interactions (%)'}\n",
    "    )\n",
    "    plt.xlabel(\"Interaction Types\", fontsize=16)\n",
    "    plt.ylabel(\"Residue Types\", fontsize=16)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=12)\n",
    "    plt.yticks(rotation=0, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"residue_interaction_heatmap.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 计算标准化分数并获取数据结构\n",
    "    residue_specific_scores, residues, interactions = calculate_normalized_scores()\n",
    "\n",
    "    # 打印残基相互作用比例\n",
    "    print(\"残基-相互作用比例表：\")\n",
    "    for residue in residues:\n",
    "        print(f\"{residue}: {', '.join([f'{interaction}={residue_specific_scores[residue][interaction]:.2f}%' for interaction in interactions])}\")\n",
    "\n",
    "    # 绘制热图\n",
    "    plot_interaction_heatmap(residue_specific_scores, residues, interactions)\n"
   ],
   "id": "1aa808a818f98170",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
